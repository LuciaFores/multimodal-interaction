{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.14)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Import all the needed libraries\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import pyaudio\n",
    "from gtts import gTTS\n",
    "from io import BytesIO\n",
    "from pygame import mixer\n",
    "import time\n",
    "import pandas as pd\n",
    "import pywhatkit as kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will be used to setup the speech recognition module\n",
    "def setup_speech_recognition():\n",
    "    # Load the model and create a recognizer\n",
    "    model = Model(\"./model/vosk-model-small-it-0.22\")\n",
    "    recognizer = KaldiRecognizer(model, 16000)\n",
    "    # setup the microphone to record audio\n",
    "    mic = pyaudio.PyAudio()\n",
    "    stream = mic.open(\n",
    "        format=pyaudio.paInt16,\n",
    "        channels=1,\n",
    "        rate=16000,\n",
    "        input=True,\n",
    "        frames_per_buffer=8192\n",
    "    )\n",
    "    stream.stop_stream()  # Start with the stream stopped\n",
    "    # return the recognizer and the stream\n",
    "    return recognizer, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will be used to setup the speech synthesis module\n",
    "def setup_speech_synthesis():\n",
    "    # setup the mixer to play the audio\n",
    "    mixer.init()\n",
    "    # return the mixer\n",
    "    return mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will be used to recognize the speech\n",
    "def recognize_speech(recognizer, stream):\n",
    "    # read the audio data from the stream\n",
    "    data = stream.read(4096)\n",
    "    # check if the data is empty\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    # check if the recognizer has recognized the speech\n",
    "    if recognizer.AcceptWaveform(data):\n",
    "        # return the recognized speech\n",
    "        return recognizer.Result()[14:-3] # remove the first 14 characters and the last 3 characters, needed to remove the metadata\n",
    "    # return None if the speech is not recognized\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will be used to synthesize the speech\n",
    "def synthesize_speech(text):\n",
    "    # create a BytesIO object to store the mp3 file\n",
    "    mp3_fp = BytesIO()\n",
    "    # create a gTTS object and write the mp3 file to the BytesIO object and so perform the synthesis\n",
    "    tts = gTTS(text, lang='it')\n",
    "    tts.write_to_fp(mp3_fp)\n",
    "    return mp3_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will be used to play the synthesized speech\n",
    "def play_speech(mixer, mp3_fp):\n",
    "    # set the BytesIO object to the beginning of the file\n",
    "    mp3_fp.seek(0)\n",
    "    # play the mp3 file\n",
    "    mixer.music.load(mp3_fp)\n",
    "    mixer.music.play()\n",
    "    # wait until the audio is played\n",
    "    while mixer.music.get_busy():\n",
    "        time.sleep(0.1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to retrieve patient data\n",
    "def get_patient_data():\n",
    "    df_registry = pd.read_csv('./patient_registry_test.csv')\n",
    "    # create a dictionary with the patient data\n",
    "    patient = {}\n",
    "    patient['name'] = df_registry['name'][0]\n",
    "    patient['gender'] = df_registry['gender'][0]\n",
    "    patient['age'] = int(df_registry['age'][0])\n",
    "    number_columns = [col for col in df_registry.columns if col.startswith('cg_number_')]\n",
    "    phone_numbers = df_registry[number_columns].values.flatten().tolist()\n",
    "    patient['phone_numbers'] = [\"+\" + str(phone_number) for phone_number in phone_numbers]\n",
    "    df_therapy_plan = pd.read_csv('./therapy_plan_test.csv')\n",
    "    # create a dictionary with the therapy plan data\n",
    "    therapy_plan = {}\n",
    "    # iterate over the therapy plan data and get only the rows for which the column medicine_1 is not empty\n",
    "    for _, row in df_therapy_plan.iterrows():\n",
    "        if not pd.isna(row['medicine_1']): # meaning that the patient must take at least one medicine at that time\n",
    "            # get all the medicines that the patient must take at that time\n",
    "            medicines = row.drop(['hour']).dropna().tolist()\n",
    "            therapy_plan[row['hour']] = medicines\n",
    "    return patient, therapy_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to greet the patient\n",
    "def greet_patient(patient):\n",
    "    # create the text to be synthesized\n",
    "    if patient['gender'] == \"F\":\n",
    "        text = f'Ciao {patient[\"name\"]}, benvenuta alla terapia vocale. Come stai oggi?'\n",
    "    else:\n",
    "        text = f'Ciao {patient[\"name\"]}, benvenuto alla terapia vocale. Come stai oggi?'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_syntesis(text, mixer, stream):\n",
    "    stream.stop_stream()\n",
    "    mp3_fp = synthesize_speech(text)\n",
    "    play_speech(mixer, mp3_fp)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to send whatsapp messages to the caregivers\n",
    "def send_whatsapp_message(patient, text):\n",
    "    phone_numbers = patient['phone_numbers']\n",
    "    for phone_number in phone_numbers:\n",
    "        #msg = f'Patient {patient[\"name\"]} recap:\\n name: {patient[\"name\"]}\\n gender: {patient[\"gender\"]}\\n age: {patient[\"age\"]}\\n message: {text}'\n",
    "        msg = text\n",
    "        kit.sendwhatmsg_instantly(phone_number, msg, tab_close=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_help_message(patient, mixer, stream):\n",
    "    send_whatsapp_message(patient, f\"{patient['name']} needs your help!\\nPlease get in contact as soon as possible!\")\n",
    "    if len(patient['phone_numbers']) > 1:\n",
    "        text = f\"Okay {patient['name']}, ho inviato un messaggio ai tuoi caregiver, ti contatteranno al più presto.\"\n",
    "    else:\n",
    "        text = f\"Okay {patient['name']}, ho inviato un messaggio al tuo caregiver, ti contatterà al più presto.\"\n",
    "    speech_syntesis(text, mixer, stream)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_therapy_plan(therapy_plan, current_time, stream, mixer):\n",
    "    text = \"è il momento di prendere i seguenti farmaci: \"\n",
    "    text += \", \".join(therapy_plan[current_time])\n",
    "    stream.stop_stream()\n",
    "    speech_syntesis(text, mixer, stream)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from ./model/vosk-model-small-it-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from ./model/vosk-model-small-it-0.22/graph/HCLr.fst ./model/vosk-model-small-it-0.22/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo ./model/vosk-model-small-it-0.22/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m stream\u001b[38;5;241m.\u001b[39mstart_stream()\n\u001b[0;32m----> 9\u001b[0m patient_speech \u001b[38;5;241m=\u001b[39m \u001b[43mrecognize_speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m patient_speech \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m patient_speech\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maiuto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     11\u001b[0m     stream\u001b[38;5;241m.\u001b[39mstop_stream()\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mrecognize_speech\u001b[0;34m(recognizer, stream)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecognize_speech\u001b[39m(recognizer, stream):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# read the audio data from the stream\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# check if the data is empty\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal_interaction/lib/python3.10/site-packages/pyaudio/__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[0;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "patient, therapy_plan = get_patient_data()\n",
    "recognizer, stream = setup_speech_recognition()\n",
    "mixer = setup_speech_synthesis()\n",
    "#patient_speech = None\n",
    "# lo stream è sempre aperto perché deve poter captare se se il paziente dice aiuto\n",
    "while True:\n",
    "    text = \"\"\n",
    "    stream.start_stream()\n",
    "    patient_speech = recognize_speech(recognizer, stream)\n",
    "    if patient_speech != None and patient_speech.startswith(\"aiuto\"):\n",
    "        stream.stop_stream()\n",
    "        send_help_message(patient, mixer, stream)\n",
    "        patient_speech = None        \n",
    "    # non sarà più se il paziente non ha parlato ma se l'orario coincide con uno degli orari del therapy plan\n",
    "    # get current time\n",
    "    current_time = time.strftime(\"%H:%M\", time.localtime())\n",
    "    # check if current time is in therapy plan.keys()\n",
    "    if current_time in therapy_plan.keys():\n",
    "        text = greet_patient(patient)\n",
    "        speech_syntesis(text, mixer, stream)\n",
    "        stream.start_stream()\n",
    "        while patient_speech == None:\n",
    "            patient_speech = recognize_speech(recognizer, stream)\n",
    "        if patient_speech.startswith(\"ben\"):\n",
    "            patient_speech = None\n",
    "            text = \"Sono contento di sentirtelo dire \" + patient[\"name\"] + \".\"\n",
    "            speech_syntesis(text, mixer, stream)\n",
    "            speech_therapy_plan(therapy_plan, current_time, stream, mixer)\n",
    "            stream.start_stream()\n",
    "            patient_speech = None\n",
    "            while patient_speech == None:\n",
    "                patient_speech = recognize_speech(recognizer, stream)\n",
    "            stream.stop_stream()\n",
    "            if patient_speech.startswith(\"grazie\"):\n",
    "                text = \"Di nulla, buona giornata \" + patient[\"name\"] + \".\"\n",
    "                speech_syntesis(text, mixer, stream)\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            text = \"Mi dispiace, spero che ti senta meglio presto.\"\n",
    "            text += \"Vuoi inviare un messaggio di aiuto?\"\n",
    "            speech_syntesis(text, mixer, stream)\n",
    "            stream.start_stream()\n",
    "            patient_speech = None\n",
    "            while patient_speech == None:\n",
    "                patient_speech = recognize_speech(recognizer, stream)\n",
    "            print(patient_speech)\n",
    "            if patient_speech == \"sì\":\n",
    "                patient_speech = None\n",
    "                send_help_message(patient, mixer, stream)\n",
    "            speech_therapy_plan(therapy_plan, current_time, stream, mixer)\n",
    "            stream.start_stream()\n",
    "            patient_speech = None\n",
    "            while patient_speech == None:\n",
    "                patient_speech = recognize_speech(recognizer, stream)\n",
    "            stream.stop_stream()\n",
    "            if patient_speech.startswith(\"grazie\"):\n",
    "                text = \"Di nulla, buona giornata \" + patient[\"name\"] + \".\"\n",
    "                speech_syntesis(text, mixer, stream)\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal_interaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
